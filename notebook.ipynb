{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengembangan dan Pengoperasian Sistem Machine Learning - Muhammad Abiya Makruf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan import library yang dibutuhkan untuk keseluruhan proyek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import os\n",
    "from typing import Text\n",
    "from absl import logging\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan set variabel seperti pipeline name, path untuk menyimpan output, path module, dan banyak lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nama pipeline\n",
    "PIPELINE_NAME = \"predicting-hiring-pipeline\"\n",
    "\n",
    "# Pipeline inputs \n",
    "DATA_ROOT = \"data\"\n",
    "TRANSFORM_MODULE_FILE = \"modules/transform.py\"\n",
    "TRAINER_MODULE_FILE = \"modules/trainer.py\"\n",
    "TUNER_MODULE_FILE = \"modules/tuner.py\"\n",
    "COMPONENTS_MODULE_FILE = \"modules/components.py\"\n",
    "\n",
    "# Pipeline outputs\n",
    "OUTPUT_BASE = \"output\"\n",
    "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
    "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
    "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pembuatan pipeline component module file menggunakan magic command. Pipeline terdiri dari:\n",
    "\n",
    "    CsvExampleGen\n",
    "    StatisticsGen\n",
    "    SchemaGen\n",
    "    ExampleValidator\n",
    "    Transform\n",
    "    Traine\n",
    "    Tuner\n",
    "    Evaluator\n",
    "    Pusher\n",
    "\n",
    "Komponen trainer sudah menggunakan komponen tuner. Pusher akan melakukan push model jika melebihi syarat dari BinaryAccuracy 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {COMPONENTS_MODULE_FILE}\n",
    "\n",
    "\"\"\"\n",
    "Author: abiyamf\n",
    "Date: 1/8/2023\n",
    "This is the components.py module.\n",
    "Usage:\n",
    "- ECreate a pipeline with TFX components.\n",
    "\"\"\"\n",
    "\n",
    "# Import library\n",
    "import os\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import (\n",
    "    CsvExampleGen, \n",
    "    StatisticsGen, \n",
    "    SchemaGen, \n",
    "    ExampleValidator, \n",
    "    Transform, \n",
    "    Trainer,\n",
    "    Tuner,\n",
    "    Evaluator,\n",
    "    Pusher\n",
    ")\n",
    "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2 \n",
    "from tfx.types import Channel\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import (\n",
    "    LatestBlessedModelStrategy)\n",
    "\n",
    "# Fungsi untuk melakukan inisialisasi components\n",
    "def init_components(config):\n",
    "\n",
    "    \"\"\"Returns tfx components for the pipeline.\n",
    " \n",
    "    Args:\n",
    "        data_dir (str): Directory containing the dataset.\n",
    "        transform_module (str): Path to the transform module.\n",
    "        tuner_module (str): Path to the tuner module.\n",
    "        training_module (str): Path to the training module.\n",
    "        training_steps (int): Number of training steps.\n",
    "        eval_steps (int): Number of evaluation steps.\n",
    "        serving_model_dir (str): Directory to save the serving\n",
    " \n",
    "    Returns:\n",
    "        components: Tuple of TFX components.\n",
    "    \"\"\" \n",
    "    \n",
    "    # Membagi dataset dengan perbandingan 8:2\n",
    "    output = example_gen_pb2.Output(\n",
    "        split_config = example_gen_pb2.SplitConfig(splits=[\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2)\n",
    "        ])\n",
    "    )\n",
    " \n",
    "    # Komponen example gen\n",
    "    example_gen = CsvExampleGen(\n",
    "        input_base=config[\"DATA_ROOT\"], \n",
    "        output_config=output\n",
    "    )\n",
    "    \n",
    "    # Komponen statistics gen\n",
    "    statistics_gen = StatisticsGen(\n",
    "        examples=example_gen.outputs[\"examples\"]   \n",
    "    )\n",
    "    \n",
    "    # Komponen schema gen\n",
    "    schema_gen = SchemaGen(\n",
    "        statistics=statistics_gen.outputs[\"statistics\"]\n",
    "    )\n",
    "    \n",
    "    # Komponen example validator\n",
    "    example_validator = ExampleValidator(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        schema=schema_gen.outputs['schema']\n",
    "    )\n",
    "    \n",
    "    # Komponen transform. Menggunakan module transform.py\n",
    "    transform  = Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema= schema_gen.outputs['schema'],\n",
    "        module_file=os.path.abspath(config[\"transform_module\"])\n",
    "    )\n",
    "\n",
    "    # Komponen tuner. Menggunakan module tuner.py\n",
    "    tuner = Tuner(\n",
    "        module_file=os.path.abspath(config[\"tuner_module\"]),\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        train_args=trainer_pb2.TrainArgs(\n",
    "            splits=['train'], \n",
    "            num_steps=config[\"training_steps\"]),\n",
    "        eval_args=trainer_pb2.EvalArgs(\n",
    "            splits=['eval'], \n",
    "            num_steps=config[\"eval_steps\"]),\n",
    "    )\n",
    "    \n",
    "    # Komponen trainer. Menggunakan module trainer.py\n",
    "    trainer  = Trainer(\n",
    "        module_file=os.path.abspath(config[\"training_module\"]),\n",
    "        examples = transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        hyperparameters=tuner.outputs['best_hyperparameters'],\n",
    "        train_args=trainer_pb2.TrainArgs(\n",
    "            splits=['train'],\n",
    "            num_steps=config[\"training_steps\"]),\n",
    "        eval_args=trainer_pb2.EvalArgs(\n",
    "            splits=['eval'], \n",
    "            num_steps=config[\"eval_steps\"])\n",
    "    )\n",
    "    \n",
    "    # Komponen model resolver\n",
    "    model_resolver = Resolver(\n",
    "        strategy_class= LatestBlessedModelStrategy,\n",
    "        model = Channel(type=Model),\n",
    "        model_blessing = Channel(type=ModelBlessing)\n",
    "    ).with_id('Latest_blessed_model_resolver')\n",
    " \n",
    "    metrics_specs = [\n",
    "        tfma.MetricsSpec(metrics=[\n",
    "                tfma.MetricConfig(class_name='AUC'),\n",
    "                tfma.MetricConfig(class_name=\"Precision\"),\n",
    "                tfma.MetricConfig(class_name=\"Recall\"),\n",
    "                tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
    "                tfma.MetricConfig(class_name='BinaryAccuracy',\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            lower_bound={'value':0.8}),\n",
    "                        change_threshold=tfma.GenericChangeThreshold(\n",
    "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                            absolute={'value':0.0001})\n",
    "                        )\n",
    "                )\n",
    "            ])\n",
    "    ]\n",
    " \n",
    "    \n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='HiringDecision')],\n",
    "        slicing_specs=[\n",
    "            tfma.SlicingSpec(),\n",
    "            ],\n",
    "        metrics_specs=metrics_specs\n",
    "    )\n",
    "    \n",
    "    # Komponen evaluator\n",
    "    evaluator = Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        eval_config=eval_config)\n",
    "    \n",
    "    # Komponen pusher\n",
    "    pusher = Pusher(\n",
    "        model=trainer.outputs[\"model\"],\n",
    "        model_blessing=evaluator.outputs[\"blessing\"],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                base_directory=config[\"serving_model_dir\"]\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Mengembalikan semua komponen\n",
    "    components = (\n",
    "        example_gen,\n",
    "        statistics_gen,\n",
    "        schema_gen,\n",
    "        example_validator,\n",
    "        transform,\n",
    "        tuner,\n",
    "        trainer,\n",
    "        model_resolver,\n",
    "        evaluator,\n",
    "        pusher\n",
    "    )\n",
    "    \n",
    "    # Mengembalikan komponen\n",
    "    return components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pembuatan module file transform menggunakan magic command. Module ini menambahkan \"_xf\" pada nama fitur dan melakukan normalisasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRANSFORM_MODULE_FILE}\n",
    "\n",
    "\"\"\"\n",
    "Author: abiyamf\n",
    "Date: 1/8/2023\n",
    "This is the transfom.py module.\n",
    "Usage:\n",
    "- Preprocess input features into transformed features.\n",
    "\"\"\"\n",
    "\n",
    "# Import library\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    " \n",
    "\n",
    "# Daftar numerical fitur pada dataset\n",
    "NUMERICAL_FEATURES = [\n",
    "    \"Age\",\n",
    "    \"Gender\",\n",
    "    \"EducationLevel\",\n",
    "    \"ExperienceYears\",\n",
    "    \"PreviousCompanies\",\n",
    "    \"DistanceFromCompany\",\n",
    "    \"InterviewScore\",\n",
    "    \"SkillScore\",\n",
    "    \"PersonalityScore\",\n",
    "    \"RecruitmentStrategy\",\n",
    "]\n",
    "\n",
    "# Label key\n",
    "LABEL_KEY = \"HiringDecision\"\n",
    " \n",
    "# Fungsi untuk mengubuah nama fitur yang sudah di transform\n",
    "def transformed_name(key):\n",
    "    \"\"\"Renaming transformed features\"\"\"\n",
    "    return key + \"_xf\"\n",
    " \n",
    "\n",
    "# Fungsi untuk melakukan preprocessing\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"\n",
    "    Preprocess input features into transformed features\n",
    "    \n",
    "    Args:\n",
    "        inputs: map from feature keys to raw features.\n",
    "    \n",
    "    Return:\n",
    "        outputs: map from feature keys to transformed features.    \n",
    "    \"\"\"\n",
    "    \n",
    "    outputs = {}\n",
    "    \n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        outputs[transformed_name(feature)] = tft.scale_to_0_1(inputs[feature])\n",
    "    \n",
    "    outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pembuatan module file tuner menggunakan magic command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/tuner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TUNER_MODULE_FILE}\n",
    "\n",
    "\"\"\"\n",
    "Author: abiyamf\n",
    "Date: 1/8/2023\n",
    "This is the trainer.py module.\n",
    "Usage:\n",
    "- Tuning the model.\n",
    "\"\"\"\n",
    "\n",
    "# Import library\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "import tensorflow_transform as tft\n",
    "import keras_tuner as kt\n",
    "from tfx.v1.components import TunerFnResult\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "from trainer import NUMERICAL_FEATURES, transformed_name, input_fn\n",
    "\n",
    "\n",
    "# Fungsi untuk membuat model\n",
    "def model_builder(hyperparameters):\n",
    "    \"\"\"\n",
    "    This function defines a Keras model and returns the model as a\n",
    "    Keras object.\n",
    "    \"\"\"\n",
    "\n",
    "    input_features = []\n",
    "\n",
    "    for key in NUMERICAL_FEATURES:\n",
    "        input_features.append(\n",
    "            tf.keras.Input(shape=(1,), name=transformed_name(key))\n",
    "        )\n",
    "\n",
    "    concatenate = tf.keras.layers.concatenate(input_features)\n",
    "    \n",
    "    deep = tf.keras.layers.Dense(hyperparameters.Choice(\n",
    "        'unit_1', [128, 256]), \n",
    "        activation=\"relu\")(concatenate)\n",
    "    deep = tf.keras.layers.Dropout(hyperparameters.Choice(\n",
    "        'dropout_1', [0.2, 0.4]))(deep)\n",
    "\n",
    "    deep = tf.keras.layers.Dense(hyperparameters.Choice(\n",
    "        'unit_2', [64, 128]), \n",
    "        activation=\"relu\")(deep)\n",
    "    deep = tf.keras.layers.Dropout(hyperparameters.Choice(\n",
    "        'dropout_2', [0.2, 0.4]))(deep)\n",
    "\n",
    "    deep = tf.keras.layers.Dense(hyperparameters.Choice(\n",
    "        'unit_3', [32, 64]), \n",
    "        activation=\"relu\")(deep)\n",
    "    deep = tf.keras.layers.Dropout(hyperparameters.Choice(\n",
    "        'dropout_3', [0.2, 0.4]))(deep)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(deep)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=input_features, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hyperparameters.Choice(\n",
    "                'learning_rate', [0.0001, 0.001])),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Fungsi tuner\n",
    "def tuner_fn(fn_args: FnArgs):\n",
    "    \"\"\"\n",
    "    Tuning the model.\n",
    "    \"\"\"\n",
    "\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    train_dataset = input_fn(fn_args.train_files, tf_transform_output, batch_size=10)\n",
    "    eval_dataset = input_fn(fn_args.eval_files, tf_transform_output, batch_size=10)\n",
    "    \n",
    "    tuner = kt.RandomSearch(\n",
    "        model_builder,\n",
    "        objective='val_binary_accuracy',\n",
    "        max_trials=10,\n",
    "        directory=fn_args.working_dir,\n",
    "        project_name='kt_hyperband'\n",
    "    )\n",
    "\n",
    "    return TunerFnResult(\n",
    "        tuner=tuner,\n",
    "        fit_kwargs={\n",
    "            \"x\": train_dataset,\n",
    "            'validation_data': eval_dataset,\n",
    "            'steps_per_epoch': fn_args.train_steps,\n",
    "            'validation_steps': fn_args.eval_steps,\n",
    "            \"epochs\": 10\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pembuatan module file trainer menggunakan magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINER_MODULE_FILE}\n",
    "\n",
    "# Import library\n",
    "\n",
    "\"\"\"\n",
    "Author: abiyamf\n",
    "Date: 1/8/2023\n",
    "This is the trainer.py module.\n",
    "Usage:\n",
    "- Define a Keras model.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow_transform as tft \n",
    " \n",
    "from transform import (\n",
    "    LABEL_KEY,\n",
    "    NUMERICAL_FEATURES,\n",
    "    transformed_name,\n",
    ")\n",
    " \n",
    "# Fungsi untuk membuat model\n",
    "def get_model(show_summary=True):\n",
    "    \"\"\"\n",
    "    This function defines a Keras model and returns the model as a\n",
    "    Keras object.\n",
    "    \"\"\"\n",
    " \n",
    "    # one-hot categorical features\n",
    "    input_features = []\n",
    "    \n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        input_features.append(\n",
    "            tf.keras.Input(shape=(1,), name=transformed_name(feature))\n",
    "        )\n",
    "    \n",
    "    concatenate = tf.keras.layers.concatenate(input_features)\n",
    "    deep = tf.keras.layers.Dense(256, activation=\"relu\")(concatenate)\n",
    "    deep = tf.keras.layers.Dense(64, activation=\"relu\")(deep)\n",
    "    deep = tf.keras.layers.Dense(16, activation=\"relu\")(deep)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(deep)\n",
    " \n",
    "    model = tf.keras.models.Model(inputs=input_features, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    \n",
    "    if show_summary:\n",
    "        model.summary()\n",
    " \n",
    "    return model\n",
    "\n",
    "# Fungsi untuk membaca data yang sudah di kompres\n",
    "def gzip_reader_fn(filenames):\n",
    "    \"\"\"Loads compressed data\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    " \n",
    "# Fungsi untuk mendapatkan fitur yang sudah di transform\n",
    "def get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \"\"\"Returns a function that parses a serialized tf.Example.\"\"\"\n",
    " \n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    " \n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(\n",
    "            serialized_tf_examples, feature_spec\n",
    "        )\n",
    " \n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    " \n",
    "        outputs = model(transformed_features)\n",
    "        return {\"outputs\": outputs}\n",
    " \n",
    "    return serve_tf_examples_fn\n",
    " \n",
    "# Fungsi untuk membuat dataset\n",
    "def input_fn(file_pattern, tf_transform_output, batch_size=64):\n",
    "    \"\"\"Generates features and labels for tuning/training.\n",
    "    Args:\n",
    "        file_pattern: input tfrecord file pattern.\n",
    "        tf_transform_output: A TFTransformOutput.\n",
    "        batch_size: representing the number of consecutive elements of \n",
    "        returned dataset to combine in a single batch\n",
    "    Returns:\n",
    "        A dataset that contains (features, indices) tuple where features\n",
    "        is a dictionary of Tensors, and indices is a single Tensor of\n",
    "        label indices.\n",
    "    \"\"\"\n",
    "    transformed_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy()\n",
    "    )\n",
    " \n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        label_key=transformed_name(LABEL_KEY),\n",
    "    )\n",
    " \n",
    "    return dataset\n",
    " \n",
    "# Fungsi untuk menjalankan model\n",
    "def run_fn(fn_args):\n",
    "    \"\"\"Train the model based on given args.\n",
    "    Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    " \n",
    "    train_dataset = input_fn(fn_args.train_files, tf_transform_output, 64)\n",
    "    eval_dataset = input_fn(fn_args.eval_files, tf_transform_output, 64)\n",
    " \n",
    "    model = get_model()\n",
    " \n",
    "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), \"logs\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir, update_freq=\"batch\"\n",
    "    )\n",
    " \n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        callbacks=[tensorboard_callback],\n",
    "        epochs=10\n",
    "    )\n",
    " \n",
    "    signatures = {\n",
    "        \"serving_default\": get_serve_tf_examples_fn(\n",
    "            model, tf_transform_output\n",
    "        ).get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\")\n",
    "        ),\n",
    "    }\n",
    "    model.save(\n",
    "        fn_args.serving_model_dir, save_format=\"tf\", signatures=signatures\n",
    "    )\n",
    "    \n",
    "    plot_model(\n",
    "        model, \n",
    "        to_file='images/model_plot.png', \n",
    "        show_shapes=True, \n",
    "        show_layer_names=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan inisialisasi local pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_local_pipeline(\n",
    "    components, pipeline_root: Text\n",
    ") -> pipeline.Pipeline:\n",
    "    \n",
    "    logging.info(f\"Pipeline root set to: {pipeline_root}\")\n",
    "    beam_args = [\n",
    "        \"--direct_running_mode=multi_processing\"\n",
    "        # 0 auto-detect based on on the number of CPUs available \n",
    "        # during execution time.\n",
    "        \"----direct_num_workers=0\" \n",
    "    ]\n",
    "    \n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "            metadata_path\n",
    "        ),\n",
    "        eam_pipeline_args=beam_args\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menjalankan pipeline menggunakan Apache Beam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 38s]\n",
      "val_binary_accuracy: 0.8632000088691711\n",
      "\n",
      "Best val_binary_accuracy So Far: 0.8715999722480774\n",
      "Total elapsed time: 00h 06m 01s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:absl:Finished tuning... Tuner ID: tuner0\n",
      "INFO:absl:Best HyperParameters: {'space': [{'class_name': 'Choice', 'config': {'name': 'unit_1', 'default': 128, 'conditions': [], 'values': [128, 256], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'dropout_1', 'default': 0.2, 'conditions': [], 'values': [0.2, 0.4], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'unit_2', 'default': 64, 'conditions': [], 'values': [64, 128], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'dropout_2', 'default': 0.2, 'conditions': [], 'values': [0.2, 0.4], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'unit_3', 'default': 32, 'conditions': [], 'values': [32, 64], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'dropout_3', 'default': 0.2, 'conditions': [], 'values': [0.2, 0.4], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'learning_rate', 'default': 0.0001, 'conditions': [], 'values': [0.0001, 0.001], 'ordered': True}}], 'values': {'unit_1': 256, 'dropout_1': 0.4, 'unit_2': 64, 'dropout_2': 0.2, 'unit_3': 64, 'dropout_3': 0.2, 'learning_rate': 0.001}}\n",
      "INFO:absl:Best Hyperparameters are written to output\\predicting-hiring-pipeline\\Tuner\\best_hyperparameters\\7\\best_hyperparameters.txt.\n",
      "INFO:absl:Tuner results are written to output\\predicting-hiring-pipeline\\Tuner\\tuner_results\\7\\tuner_results.json.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 7 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'tuner_results': [Artifact(artifact: uri: \"output\\\\predicting-hiring-pipeline\\\\Tuner\\\\tuner_results\\\\7\"\n",
      ", artifact_type: name: \"TunerResults\"\n",
      ")], 'best_hyperparameters': [Artifact(artifact: uri: \"output\\\\predicting-hiring-pipeline\\\\Tuner\\\\best_hyperparameters\\\\7\"\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}) for execution 7\n",
      "INFO:absl:MetadataStore with DB connection initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in output\\predicting-hiring-pipeline\\Tuner\\.system\\executor_execution\\7\\.temp\\7\\kt_hyperband\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_binary_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "unit_1: 256\n",
      "dropout_1: 0.4\n",
      "unit_2: 64\n",
      "dropout_2: 0.2\n",
      "unit_3: 64\n",
      "dropout_3: 0.2\n",
      "learning_rate: 0.001\n",
      "Score: 0.8715999722480774\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "unit_1: 256\n",
      "dropout_1: 0.4\n",
      "unit_2: 128\n",
      "dropout_2: 0.4\n",
      "unit_3: 32\n",
      "dropout_3: 0.4\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8712000250816345\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "unit_1: 256\n",
      "dropout_1: 0.4\n",
      "unit_2: 64\n",
      "dropout_2: 0.2\n",
      "unit_3: 32\n",
      "dropout_3: 0.2\n",
      "learning_rate: 0.001\n",
      "Score: 0.8708000183105469\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "unit_1: 256\n",
      "dropout_1: 0.4\n",
      "unit_2: 64\n",
      "dropout_2: 0.4\n",
      "unit_3: 32\n",
      "dropout_3: 0.4\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8691999912261963\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "unit_1: 256\n",
      "dropout_1: 0.2\n",
      "unit_2: 128\n",
      "dropout_2: 0.2\n",
      "unit_3: 64\n",
      "dropout_3: 0.2\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8676000237464905\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "unit_1: 256\n",
      "dropout_1: 0.4\n",
      "unit_2: 128\n",
      "dropout_2: 0.2\n",
      "unit_3: 64\n",
      "dropout_3: 0.4\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8676000237464905\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "unit_1: 128\n",
      "dropout_1: 0.4\n",
      "unit_2: 128\n",
      "dropout_2: 0.2\n",
      "unit_3: 64\n",
      "dropout_3: 0.4\n",
      "learning_rate: 0.0001\n",
      "Score: 0.864799976348877\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "unit_1: 256\n",
      "dropout_1: 0.4\n",
      "unit_2: 64\n",
      "dropout_2: 0.4\n",
      "unit_3: 64\n",
      "dropout_3: 0.2\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8632000088691711\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "unit_1: 128\n",
      "dropout_1: 0.4\n",
      "unit_2: 128\n",
      "dropout_2: 0.4\n",
      "unit_3: 32\n",
      "dropout_3: 0.4\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8632000088691711\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "unit_1: 128\n",
      "dropout_1: 0.2\n",
      "unit_2: 128\n",
      "dropout_2: 0.2\n",
      "unit_3: 32\n",
      "dropout_3: 0.4\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8619999885559082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:node Tuner is finished.\n",
      "INFO:absl:node Trainer is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"predicting-hiring-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20240801-192334.648142\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"predicting-hiring-pipeline.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transformed_examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Tuner\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Tuner\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"HyperParameters\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"best_hyperparameters\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"TransformGraph\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transform_graph\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 250,\\n  \\\"splits\\\": [\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"trainer@output\\\\predicting-hiring-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+56572d4e8c14ac05bbb00442804984ef31a5f0f30a9e1f048a084d44e088d7ef-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 1000,\\n  \\\"splits\\\": [\\n    \\\"train\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "upstream_nodes: \"Transform\"\n",
      "upstream_nodes: \"Tuner\"\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Trainer] Resolved inputs: ({'transform_graph': [Artifact(artifact: id: 11\n",
      "type_id: 25\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\Transform\\\\transform_graph\\\\6\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515062594\n",
      "last_update_time_since_epoch: 1722515062594\n",
      ", artifact_type: id: 25\n",
      "name: \"TransformGraph\"\n",
      ")], 'examples': [Artifact(artifact: id: 6\n",
      "type_id: 16\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\Transform\\\\transformed_examples\\\\6\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515062593\n",
      "last_update_time_since_epoch: 1722515062593\n",
      ", artifact_type: id: 16\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'hyperparameters': [Artifact(artifact: id: 14\n",
      "type_id: 28\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\Tuner\\\\best_hyperparameters\\\\7\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515428370\n",
      "last_update_time_since_epoch: 1722515428370\n",
      ", artifact_type: id: 28\n",
      "name: \"HyperParameters\"\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 20\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\SchemaGen\\\\schema\\\\4\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515024722\n",
      "last_update_time_since_epoch: 1722515024722\n",
      ", artifact_type: id: 20\n",
      "name: \"Schema\"\n",
      ")]},)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 8\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'transform_graph': [Artifact(artifact: id: 11\n",
      "type_id: 25\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\Transform\\\\transform_graph\\\\6\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515062594\n",
      "last_update_time_since_epoch: 1722515062594\n",
      ", artifact_type: id: 25\n",
      "name: \"TransformGraph\"\n",
      ")], 'examples': [Artifact(artifact: id: 6\n",
      "type_id: 16\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\Transform\\\\transformed_examples\\\\6\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515062593\n",
      "last_update_time_since_epoch: 1722515062593\n",
      ", artifact_type: id: 16\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'hyperparameters': [Artifact(artifact: id: 14\n",
      "type_id: 28\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\Tuner\\\\best_hyperparameters\\\\7\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515428370\n",
      "last_update_time_since_epoch: 1722515428370\n",
      ", artifact_type: id: 28\n",
      "name: \"HyperParameters\"\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 20\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\SchemaGen\\\\schema\\\\4\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515024722\n",
      "last_update_time_since_epoch: 1722515024722\n",
      ", artifact_type: id: 20\n",
      "name: \"Schema\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"output\\\\predicting-hiring-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_run': [Artifact(artifact: uri: \"output\\\\predicting-hiring-pipeline\\\\Trainer\\\\model_run\\\\8\"\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")]}), exec_properties={'eval_args': '{\\n  \"num_steps\": 250,\\n  \"splits\": [\\n    \"eval\"\\n  ]\\n}', 'module_path': 'trainer@output\\\\predicting-hiring-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+56572d4e8c14ac05bbb00442804984ef31a5f0f30a9e1f048a084d44e088d7ef-py3-none-any.whl', 'custom_config': 'null', 'train_args': '{\\n  \"num_steps\": 1000,\\n  \"splits\": [\\n    \"train\"\\n  ]\\n}'}, execution_output_uri='output\\\\predicting-hiring-pipeline\\\\Trainer\\\\.system\\\\executor_execution\\\\8\\\\executor_output.pb', stateful_working_dir='output\\\\predicting-hiring-pipeline\\\\Trainer\\\\.system\\\\stateful_working_dir\\\\20240801-192334.648142', tmp_dir='output\\\\predicting-hiring-pipeline\\\\Trainer\\\\.system\\\\executor_execution\\\\8\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"predicting-hiring-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20240801-192334.648142\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"predicting-hiring-pipeline.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transformed_examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Tuner\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Tuner\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"HyperParameters\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"best_hyperparameters\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"TransformGraph\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transform_graph\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 250,\\n  \\\"splits\\\": [\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"trainer@output\\\\predicting-hiring-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+56572d4e8c14ac05bbb00442804984ef31a5f0f30a9e1f048a084d44e088d7ef-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 1000,\\n  \\\"splits\\\": [\\n    \\\"train\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "upstream_nodes: \"Transform\"\n",
      "upstream_nodes: \"Tuner\"\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"predicting-hiring-pipeline\"\n",
      ", pipeline_run_id='20240801-192334.648142')\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "INFO:absl:udf_utils.get_fn {'eval_args': '{\\n  \"num_steps\": 250,\\n  \"splits\": [\\n    \"eval\"\\n  ]\\n}', 'module_path': 'trainer@output\\\\predicting-hiring-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+56572d4e8c14ac05bbb00442804984ef31a5f0f30a9e1f048a084d44e088d7ef-py3-none-any.whl', 'custom_config': 'null', 'train_args': '{\\n  \"num_steps\": 1000,\\n  \"splits\": [\\n    \"train\"\\n  ]\\n}'} 'run_fn'\n",
      "INFO:absl:Installing 'output\\\\predicting-hiring-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+56572d4e8c14ac05bbb00442804984ef31a5f0f30a9e1f048a084d44e088d7ef-py3-none-any.whl' to a temporary directory.\n",
      "INFO:absl:Executing: ['c:\\\\Users\\\\ABIYA\\\\anaconda3\\\\envs\\\\proyek-akhir-mlops\\\\python.exe', '-m', 'pip', 'install', '--target', 'C:\\\\Users\\\\ABIYA\\\\AppData\\\\Local\\\\Temp\\\\tmp7smn8nqu', 'output\\\\predicting-hiring-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+56572d4e8c14ac05bbb00442804984ef31a5f0f30a9e1f048a084d44e088d7ef-py3-none-any.whl']\n",
      "INFO:absl:Successfully installed 'output\\\\predicting-hiring-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+56572d4e8c14ac05bbb00442804984ef31a5f0f30a9e1f048a084d44e088d7ef-py3-none-any.whl'.\n",
      "INFO:absl:Training model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Age_xf (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Gender_xf (InputLayer)         [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " EducationLevel_xf (InputLayer)  [(None, 1)]         0           []                               \n",
      "                                                                                                  \n",
      " ExperienceYears_xf (InputLayer  [(None, 1)]         0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " PreviousCompanies_xf (InputLay  [(None, 1)]         0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " DistanceFromCompany_xf (InputL  [(None, 1)]         0           []                               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " InterviewScore_xf (InputLayer)  [(None, 1)]         0           []                               \n",
      "                                                                                                  \n",
      " SkillScore_xf (InputLayer)     [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " PersonalityScore_xf (InputLaye  [(None, 1)]         0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " RecruitmentStrategy_xf (InputL  [(None, 1)]         0           []                               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 10)           0           ['Age_xf[0][0]',                 \n",
      "                                                                  'Gender_xf[0][0]',              \n",
      "                                                                  'EducationLevel_xf[0][0]',      \n",
      "                                                                  'ExperienceYears_xf[0][0]',     \n",
      "                                                                  'PreviousCompanies_xf[0][0]',   \n",
      "                                                                  'DistanceFromCompany_xf[0][0]', \n",
      "                                                                  'InterviewScore_xf[0][0]',      \n",
      "                                                                  'SkillScore_xf[0][0]',          \n",
      "                                                                  'PersonalityScore_xf[0][0]',    \n",
      "                                                                  'RecruitmentStrategy_xf[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          2816        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           16448       ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 16)           1040        ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            17          ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,321\n",
      "Trainable params: 20,321\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.2726 - binary_accuracy: 0.9011 - val_loss: 0.4376 - val_binary_accuracy: 0.8679\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.0991 - binary_accuracy: 0.9683 - val_loss: 0.7470 - val_binary_accuracy: 0.8482\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.0222 - binary_accuracy: 0.9948 - val_loss: 1.1340 - val_binary_accuracy: 0.8649\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.0030 - binary_accuracy: 1.0000 - val_loss: 1.4406 - val_binary_accuracy: 0.8711\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 8.3760e-04 - binary_accuracy: 1.0000 - val_loss: 1.6417 - val_binary_accuracy: 0.8616\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.0993e-04 - binary_accuracy: 1.0000 - val_loss: 1.8277 - val_binary_accuracy: 0.8680\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.4049e-04 - binary_accuracy: 1.0000 - val_loss: 1.9864 - val_binary_accuracy: 0.8679\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 6.6833e-05 - binary_accuracy: 1.0000 - val_loss: 2.1500 - val_binary_accuracy: 0.8648\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3491e-05 - binary_accuracy: 1.0000 - val_loss: 2.2968 - val_binary_accuracy: 0.8680\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.7433e-05 - binary_accuracy: 1.0000 - val_loss: 2.4287 - val_binary_accuracy: 0.8712\n",
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output\\predicting-hiring-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output\\predicting-hiring-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Training complete. Model written to output\\predicting-hiring-pipeline\\Trainer\\model\\8\\Format-Serving. ModelRun written to output\\predicting-hiring-pipeline\\Trainer\\model_run\\8\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 8 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"output\\\\predicting-hiring-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_run': [Artifact(artifact: uri: \"output\\\\predicting-hiring-pipeline\\\\Trainer\\\\model_run\\\\8\"\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")]}) for execution 8\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:node Trainer is finished.\n",
      "INFO:absl:node Evaluator is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "    base_type: EVALUATE\n",
      "  }\n",
      "  id: \"Evaluator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"predicting-hiring-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20240801-192334.648142\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"predicting-hiring-pipeline.Evaluator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"baseline_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Latest_blessed_model_resolver\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Latest_blessed_model_resolver\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"blessing\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelBlessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"evaluation\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelEvaluation\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"eval_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"AUC\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Precision\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Recall\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": 0.0001,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"HiringDecision\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"example_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"fairness_indicator_thresholds\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"Latest_blessed_model_resolver\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Evaluator] Resolved inputs: ({'model': [Artifact(artifact: id: 15\n",
      "type_id: 30\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515482031\n",
      "last_update_time_since_epoch: 1722515482031\n",
      ", artifact_type: id: 30\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'examples': [Artifact(artifact: id: 1\n",
      "type_id: 16\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\CsvExampleGen\\\\examples\\\\2\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:63964,xor_checksum:1718633630,sum_checksum:1718633630\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515018138\n",
      "last_update_time_since_epoch: 1722515018138\n",
      ", artifact_type: id: 16\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'baseline_model': []},)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 9\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'model': [Artifact(artifact: id: 15\n",
      "type_id: 30\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515482031\n",
      "last_update_time_since_epoch: 1722515482031\n",
      ", artifact_type: id: 30\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'examples': [Artifact(artifact: id: 1\n",
      "type_id: 16\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\CsvExampleGen\\\\examples\\\\2\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:63964,xor_checksum:1718633630,sum_checksum:1718633630\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515018138\n",
      "last_update_time_since_epoch: 1722515018138\n",
      ", artifact_type: id: 16\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'baseline_model': []}, output_dict=defaultdict(<class 'list'>, {'blessing': [Artifact(artifact: uri: \"output\\\\predicting-hiring-pipeline\\\\Evaluator\\\\blessing\\\\9\"\n",
      ", artifact_type: name: \"ModelBlessing\"\n",
      ")], 'evaluation': [Artifact(artifact: uri: \"output\\\\predicting-hiring-pipeline\\\\Evaluator\\\\evaluation\\\\9\"\n",
      ", artifact_type: name: \"ModelEvaluation\"\n",
      ")]}), exec_properties={'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"HiringDecision\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}'}, execution_output_uri='output\\\\predicting-hiring-pipeline\\\\Evaluator\\\\.system\\\\executor_execution\\\\9\\\\executor_output.pb', stateful_working_dir='output\\\\predicting-hiring-pipeline\\\\Evaluator\\\\.system\\\\stateful_working_dir\\\\20240801-192334.648142', tmp_dir='output\\\\predicting-hiring-pipeline\\\\Evaluator\\\\.system\\\\executor_execution\\\\9\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "    base_type: EVALUATE\n",
      "  }\n",
      "  id: \"Evaluator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"predicting-hiring-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20240801-192334.648142\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"predicting-hiring-pipeline.Evaluator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"baseline_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Latest_blessed_model_resolver\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Latest_blessed_model_resolver\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"blessing\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelBlessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"evaluation\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelEvaluation\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"eval_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"AUC\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Precision\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Recall\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": 0.0001,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"HiringDecision\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"example_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"fairness_indicator_thresholds\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"Latest_blessed_model_resolver\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"predicting-hiring-pipeline\"\n",
      ", pipeline_run_id='20240801-192334.648142')\n",
      "INFO:absl:udf_utils.get_fn {'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"HiringDecision\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}'} 'custom_eval_shared_model'\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"HiringDecision\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using output\\predicting-hiring-pipeline\\Trainer\\model\\8\\Format-Serving as  model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0CF557340> and <keras.engine.input_layer.InputLayer object at 0x000001C0C2F79EB0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0CF557340> and <keras.engine.input_layer.InputLayer object at 0x000001C0C2F79EB0>).\n",
      "INFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\n",
      "INFO:absl:Evaluating model.\n",
      "INFO:absl:udf_utils.get_fn {'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"HiringDecision\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}'} 'custom_extractors'\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"HiringDecision\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "}\n",
      "\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"HiringDecision\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "}\n",
      "\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"HiringDecision\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0CDA4B310> and <keras.engine.input_layer.InputLayer object at 0x000001C0DC9C7040>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0CDA4B310> and <keras.engine.input_layer.InputLayer object at 0x000001C0DC9C7040>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0DB8DDC70> and <keras.engine.input_layer.InputLayer object at 0x000001C0DB8B5C10>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0DB8DDC70> and <keras.engine.input_layer.InputLayer object at 0x000001C0DB8B5C10>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0D7F25730> and <keras.engine.input_layer.InputLayer object at 0x000001C0D7F53F40>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0D7F25730> and <keras.engine.input_layer.InputLayer object at 0x000001C0D7F53F40>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0D7C83190> and <keras.engine.input_layer.InputLayer object at 0x000001C0D7F51F10>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0D7C83190> and <keras.engine.input_layer.InputLayer object at 0x000001C0D7F51F10>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0DCC3D430> and <keras.engine.input_layer.InputLayer object at 0x000001C0DCC2CF70>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0DCC3D430> and <keras.engine.input_layer.InputLayer object at 0x000001C0DCC2CF70>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0CF11C070> and <keras.engine.input_layer.InputLayer object at 0x000001C0CDBC30A0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0CF11C070> and <keras.engine.input_layer.InputLayer object at 0x000001C0CDBC30A0>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0C66B0490> and <keras.engine.input_layer.InputLayer object at 0x000001C0C2EEB850>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C0C66B0490> and <keras.engine.input_layer.InputLayer object at 0x000001C0C2EEB850>).\n",
      "INFO:absl:Evaluation complete. Results written to output\\predicting-hiring-pipeline\\Evaluator\\evaluation\\9.\n",
      "INFO:absl:Checking validation results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ABIYA\\anaconda3\\envs\\proyek-akhir-mlops\\lib\\site-packages\\tensorflow_model_analysis\\writers\\metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ABIYA\\anaconda3\\envs\\proyek-akhir-mlops\\lib\\site-packages\\tensorflow_model_analysis\\writers\\metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:absl:Blessing result True written to output\\predicting-hiring-pipeline\\Evaluator\\blessing\\9.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 9 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'blessing': [Artifact(artifact: uri: \"output\\\\predicting-hiring-pipeline\\\\Evaluator\\\\blessing\\\\9\"\n",
      ", artifact_type: name: \"ModelBlessing\"\n",
      ")], 'evaluation': [Artifact(artifact: uri: \"output\\\\predicting-hiring-pipeline\\\\Evaluator\\\\evaluation\\\\9\"\n",
      ", artifact_type: name: \"ModelEvaluation\"\n",
      ")]}) for execution 9\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:node Evaluator is finished.\n",
      "INFO:absl:node Pusher is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"predicting-hiring-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20240801-192334.648142\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"predicting-hiring-pipeline.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Evaluator\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Evaluator\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"output\\\\\\\\serving_model\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Evaluator\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Pusher] Resolved inputs: ({'model': [Artifact(artifact: id: 15\n",
      "type_id: 30\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515482031\n",
      "last_update_time_since_epoch: 1722515482031\n",
      ", artifact_type: id: 30\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_blessing': [Artifact(artifact: id: 17\n",
      "type_id: 33\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\Evaluator\\\\blessing\\\\9\"\n",
      "custom_properties {\n",
      "  key: \"blessed\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model\"\n",
      "  value {\n",
      "    string_value: \"output\\\\predicting-hiring-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model_id\"\n",
      "  value {\n",
      "    int_value: 15\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515494914\n",
      "last_update_time_since_epoch: 1722515494914\n",
      ", artifact_type: id: 33\n",
      "name: \"ModelBlessing\"\n",
      ")]},)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 10\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=10, input_dict={'model': [Artifact(artifact: id: 15\n",
      "type_id: 30\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515482031\n",
      "last_update_time_since_epoch: 1722515482031\n",
      ", artifact_type: id: 30\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_blessing': [Artifact(artifact: id: 17\n",
      "type_id: 33\n",
      "uri: \"output\\\\predicting-hiring-pipeline\\\\Evaluator\\\\blessing\\\\9\"\n",
      "custom_properties {\n",
      "  key: \"blessed\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model\"\n",
      "  value {\n",
      "    string_value: \"output\\\\predicting-hiring-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model_id\"\n",
      "  value {\n",
      "    int_value: 15\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1722515494914\n",
      "last_update_time_since_epoch: 1722515494914\n",
      ", artifact_type: id: 33\n",
      "name: \"ModelBlessing\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"output\\\\predicting-hiring-pipeline\\\\Pusher\\\\pushed_model\\\\10\"\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}), exec_properties={'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"output\\\\\\\\serving_model\"\\n  }\\n}', 'custom_config': 'null'}, execution_output_uri='output\\\\predicting-hiring-pipeline\\\\Pusher\\\\.system\\\\executor_execution\\\\10\\\\executor_output.pb', stateful_working_dir='output\\\\predicting-hiring-pipeline\\\\Pusher\\\\.system\\\\stateful_working_dir\\\\20240801-192334.648142', tmp_dir='output\\\\predicting-hiring-pipeline\\\\Pusher\\\\.system\\\\executor_execution\\\\10\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"predicting-hiring-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20240801-192334.648142\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"predicting-hiring-pipeline.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Evaluator\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20240801-192334.648142\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"predicting-hiring-pipeline.Evaluator\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"output\\\\\\\\serving_model\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Evaluator\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"predicting-hiring-pipeline\"\n",
      ", pipeline_run_id='20240801-192334.648142')\n",
      "INFO:absl:Model version: 1722515495\n",
      "INFO:absl:Model written to serving path output\\serving_model\\1722515495.\n",
      "INFO:absl:Model pushed to output\\predicting-hiring-pipeline\\Pusher\\pushed_model\\10.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 10 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"output\\\\predicting-hiring-pipeline\\\\Pusher\\\\pushed_model\\\\10\"\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}) for execution 10\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:node Pusher is finished.\n"
     ]
    }
   ],
   "source": [
    "from modules.components import init_components\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "\n",
    "config = {\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"training_module\": TRAINER_MODULE_FILE,\n",
    "    \"transform_module\": TRANSFORM_MODULE_FILE,\n",
    "    \"tuner_module\": TUNER_MODULE_FILE,\n",
    "    \"training_steps\": 1000,\n",
    "    \"eval_steps\": 250,\n",
    "    \"serving_model_dir\": serving_model_dir,\n",
    "}\n",
    "\n",
    "components = init_components(config)\n",
    "\n",
    "pipeline = init_local_pipeline(components, pipeline_root)\n",
    "BeamDagRunner().run(pipeline=pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyek-akhir-mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
